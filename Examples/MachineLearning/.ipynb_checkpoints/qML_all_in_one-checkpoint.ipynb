{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 20px; border-radius: 10px ; border: 2px solid green;\">\n",
    "<p><font size=\"+3\"><b><center> qcware.qML </center></b></font>\n",
    "</p>\n",
    "<p>\n",
    "<font size=\"+3\">  <b> <center> The Quantum Machine Learning API</center></b></font>\n",
    "</p>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**qcware.qML** is Forge's API for Machine Learning applications. \n",
    "\n",
    "Our ambition is to deliver the first real Quantum Machine Learning applications by bridging the gap between higly impactful QML algorithms and the NISQ era quantum machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why QML?** \n",
    "\n",
    "First because ML provides extremely impactful tools for a number of different domains (Finance, Healthcare, Automotive, etc.), so it only makes sense to try as hard as possible and see how quantum can enhance these tools to offer Efficiency, Accuracy, Explainability, Trustworthiness, Energy savings etc. \n",
    "\n",
    "Second, we do know fault tolerant quantum computers can offer big advantages in ML, and we have contributed largely in the growing body of work in this area. \n",
    "\n",
    "Third, because we do have concrete avenues for bringing QML towards the NISQ era: by reducing resource requirements of impactful QML algorithms; by proposing NISQ Quantum Deep Learning with provable guarantees; even by pushing the boundaries of NISQ technologies to new QML-specific hardware architectures for overcoming bottlenecks.\n",
    "\n",
    "Our QML methodology is NOT to nullify the algorithmic intuition and use blindly current hardware. It is both to push the boundaries of NISQ hardware and to reduce the algorithmic needs to get to the first real-world QML applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we do this?**\n",
    "\n",
    "We needed to first solve a number of bottlenecks and get a brand new engine under the hood. How do we efficiently load classical data into quantum states? How do we extract useful information from quantum solutions fast? How do we calculate the similarity between data points, users, stock histories, patients on the fly? These are our propriatory \"under the hood\" developments that make our API stronger than ever and ready for a test drive. \n",
    "\n",
    "In the current release, we use our new powerful quantum tools for supervised and unsupervised learning and provide NISQ solutions for classification, regression and clustering.\n",
    "\n",
    "The target audience of Forge is both the users who want to solve directly their use cases using our optimized quantum functionalities without having to worry too much about the quantum tools underneath, and also the users who want to start experimenting by themselves with building small quantum circuits and functions. \n",
    "\n",
    "But the best way to do this is to do this together! Some of our more powerful propriatory quantum tools make sense to be available in a more \"supervised\" setting, where QC Ware quantum algorithm experts accompany the users through Proof of Concept projects to step by step understand the power and limitations of these tools and how they can be optimally applied to each specific use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qcware.qml import fit_and_predict\n",
    "# as always when using Forge, we must set our API key if not using the hosted notebooks\n",
    "import qcware\n",
    "# qcware.config.set_api_key('QCWARE')\n",
    "# if scikit-learn is not installed, install it; it should be installed on the Forge notebook servers\n",
    "!pip install sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 10px; border-radius: 10px ; border: 1px solid green;\">\n",
    "<font size=\"+2\"><b> 0. Under the hood </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's take a peek on what we built under the hood to ensure that we move closer towards these first QML applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 0.5px solid green;\">\n",
    "<font size=\"+1\"><b> 0.1 quasar </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**quasar** is our internal quantum language which for a user who has experimented with other quantum languages will seem very intuitive and similar to use. It is a simple circuit writing tool that offers some big advantages: \n",
    "\n",
    "- Design your circuit once, run it on all available simulators and quantum hardware \n",
    "- Use powerful built-in functionalities specifically for Optimization, Chemistry and ML\n",
    "- Multiple backends supported\n",
    "- GPU powered simulators available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 0.2 Data loaders </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first obstacles towards practical QML was loading efficiently classical data onto quantum states that can be used for further computation.\n",
    "Why do we need that? Well, because ML data IS and will predominantly remain classical (images, texts, preferences, stock market, internet data,...) so if we are looking for impactful quantum applications, ignoring classical data is not an option!\n",
    "\n",
    "So what do we mean by **efficiently** loading classical data onto quantum states? We certainly  do not want to assume any exotic hardware technologies that are not or will not be available in the NISQ era. And more importantly we want to be able to create amplitude encodings of our classical data points as fast as possible, namely we want to be able to load classical data in the following sense:  \n",
    "\n",
    "    - Read the classical data once! (time: O(n), for n-dimensional real-valued data)\n",
    "    - Prepare quantum encodings of the data fast! (time: logn, circuit size (2-q gates): n)\n",
    "\n",
    "And we deliver exactly that. We have designed data loaders that:\n",
    "\n",
    "- loader(): Provide the **optimal** ways for loading classical data onto quantum states\n",
    "- Can be deployed using current NISQ machines\n",
    "- Can be readily used as subroutines to bring many QML algorithms closer to the NISQ era.  \n",
    "\n",
    "We are also discussing with quantum hardware collaborators to design tailor-made hardware chips for even better performance and seemless integration. Stay tuned!\n",
    "\n",
    "Our standard Forge users will be able to get advantage of our data loaders through the qML functionalities that we are offering both in the supervised and unsupervised learning. For the ones who would really want to spend more time and effort to understand the inner workings of the loaders and our other quantum tools, we are happy to accompany you on this journey. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Here is how the loaders work in very high level. Define your data point x as an 1d array. Normalize it. Call the loader to produce the circuit to create the quantum amplitude encoding of x. As simple as that. You can see how many qubits, what depth or how many gates the loader uses. Here is some sample code of what we do under the hood. These loaders are going to be used when you run your own QML applications in the sections below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # create a synthetic data point or upload your own data!\n",
    "\n",
    "    import numpy as np\n",
    "    x = np.random.rand(64)\n",
    "    x = x/np.linalg.norm(x)\n",
    "\n",
    "    # create a quasar circuit and call the loader function.\n",
    "    # The loader function takes as input the classical data point\n",
    "    # and mode 'parallel' or 'optimized' and it returns the quantum circuit\n",
    "\n",
    "    parallel_loader = qio.loader(x, mode='parallel')\n",
    "\n",
    "    # let's see what characteristics the circuit has\n",
    "    print(\"Parallel loader characteristics\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"number of qubits:\",parallel_loader.nqubit)\n",
    "    print(\"circuit depth   :\",parallel_loader.ntime)\n",
    "    print(\"number of gates :\",parallel_loader.ngate,\"\\n\")\n",
    "\n",
    "\n",
    "    # let's do it for the 'optimized' loader as well\n",
    "\n",
    "    optimized_loader = qio.loader(x,mode='optimized')\n",
    "\n",
    "    print(\"Optimized loader characteristics\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"number of qubits:\",optimized_loader.nqubit)\n",
    "    print(\"circuit depth   :\",optimized_loader.ntime)\n",
    "    print(\"number of gates :\",optimized_loader.ngate,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the characteristics of the loaders for 64-dimensional data\n",
    "    \n",
    "    >>> Parallel loader characteristics\n",
    "    >>> -------------------------------\n",
    "    >>> number of qubits: 64\n",
    "    >>> circuit depth   : 7\n",
    "    >>> number of gates : 64 \n",
    "    >>> \n",
    "    >>> Optimized loader characteristics\n",
    "    >>> -------------------------------\n",
    "    >>> number of qubits: 16\n",
    "    >>> circuit depth   : 34\n",
    "    >>> number of gates : 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 0.3 Distance Metrics Estimation </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason to create these data loaders is because they unlock a throve of important capabilities, the first of which is estimating the similarity or distance between data points in a fast way. Distance metrics are at the core of Similarity Learning, which is one of the fundamental branches of Supervised and Unsupervised Learning that provides accurate, efficient, and explainable AI applications.\n",
    "\n",
    "Our Distance Metrics estimation functionalities combine in a clever way our loaders, and not much more, to create NISQ subroutines for Similarity Learning. More precisely, we have developed distance metrics subroutines that:\n",
    "\n",
    "- distance_estimation(): Provide **optimal** Euclidean distance estimation of data points\n",
    "- qdot(): Provide **optimal** Inner Product estimation between data points\n",
    "- Can be deployed using current NISQ machines\n",
    "- Can be readily used as subroutines to bring many QML algorithms closer to the NISQ era.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "Here it is how it works. Define your data points x and y as 1d arrays. Call the distance_estimation prodecure to get an estimate of their squared Euclidean distance. The distance_estimation procedure constructs a quantum circuit and uses the designated backend (here the quasar simulator) to run the quantum circuit, get measurement results and output the estimate. Again, these circuits are going to be used when you run your own QML applications in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # create two random data points or load your own data\n",
    "    \n",
    "    import numpy as np\n",
    "    x = np.random.rand(16)\n",
    "    y = np.random.rand(16)\n",
    "\n",
    "    # let's estimate the distance and compare it to the real one\n",
    "    \n",
    "    print('real distance',np.linalg.norm(x-y)**2)\n",
    "    print('distance est.',qutils.distance_estimation(x, y,loader_mode='parallel'))\n",
    "\n",
    "    # we can also see what characteristics the circuit has\n",
    "\n",
    "    print(\"\\nDistance Estimation characteristics\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"number of qubits:\",dist_est.nqubit)\n",
    "    print(\"circuit depth:\",dist_est.ntime)\n",
    "    print(\"number of gates\",dist_est.ngate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample output and the characteristics of the quantum circuit for 16-dimensional data\n",
    "    \n",
    "    >>> real distance 2.9635211340099668\n",
    "    >>> distance est. 2.96147527209646\n",
    "    >>>\n",
    "    >>> Distance Estimation characteristics\n",
    "    >>> -------------------------------\n",
    "    >>> number of qubits: 17\n",
    "    >>> circuit depth: 13\n",
    "    >>> number of gates 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the inner product between vectors or matrices in the same way. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # create two random data matrices or load your own data\n",
    "    import numpy as np\n",
    "    x = np.random.rand(4,5)\n",
    "    y = np.random.rand(5,3)\n",
    "\n",
    "    # compute the dot product of the two matrices\n",
    "    # and the quantum dot product estimation of the two matrices\n",
    "    \n",
    "    print(np.dot(x,y))\n",
    "    print(qutils.qdot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample outcome\n",
    "    \n",
    "    >>> [[1.68310796 0.95850939 1.23604967]\n",
    "    >>>  [1.71744922 1.08271752 1.4903697 ]\n",
    "    >>>  [1.01983769 0.58927693 0.74201257]\n",
    "    >>>  [0.61285301 0.24302302 0.88084614]]\n",
    "    >>> [[1.68568646 0.95377326 1.25345586]\n",
    "    >>>  [1.71279657 1.07006217 1.4773043 ]\n",
    "    >>>  [1.01892272 0.58610302 0.74277502]\n",
    "    >>>  [0.61225585 0.24286683 0.88671553]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 0.4 Datasets and Visualization </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of ways to load datasets and visualize the results. Any method that works with scikit-learn will also work with our QML algorithms. Be careful, quantum simulations run out of memory pretty fast!\n",
    "\n",
    "Here we just provide some simple code for generating synthetic data and plotting the results. Nothing extraordinary, but an easy way to start playing with our QML functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data_clusters(n_clusters = 4, n_points = 8, dimension = 2, magnitude = 1, spread = 0.05, min_distance = 0.3, add_labels = False):\n",
    "    \"\"\"\n",
    "    Generates data clusters containing npoints, with random centres and spreads.\n",
    "    \n",
    "    Args:\n",
    "     n_clusters (int): Number of clusters to create\n",
    "     n_points (numpy.array): array of number of points in each cluster. \n",
    "                             If int, same number of points in each cluster\n",
    "     dimension (int): Number of features in data.\n",
    "     magnitude: max magnitude of data points\n",
    "     spread: spread of the normal distribution\n",
    "     min_dist: minimum distance between cluster centers\n",
    "     add_labels: True for supervised data, False for unsupervised data\n",
    "    \"\"\"\n",
    "    if type(n_points) == int:\n",
    "        n_points = np.tile(n_points,n_clusters)\n",
    "     \n",
    "    clusters = []\n",
    "    \n",
    "    if min_distance > 0:\n",
    "        means = []\n",
    "        while len(means) < n_clusters:\n",
    "            mean = np.random.random(dimension) * magnitude \n",
    "            bools = []\n",
    "\n",
    "            for i in range(len(means)):\n",
    "                if np.linalg.norm(mean - means[i]) > min_distance:\n",
    "                   bools.append(True)\n",
    "                else: bools.append(False)\n",
    "            if bools.count(False) > 0: \n",
    "                pass\n",
    "            else: \n",
    "                means.append(mean)\n",
    "                 \n",
    "    else: means = np.random.random((n_clusters,dimension)) * magnitude\n",
    "                \n",
    "    for i in range(n_clusters):\n",
    "        mean = np.array(means)[i]\n",
    "        cov = np.identity(dimension) * spread\n",
    "        clusters.append(np.random.multivariate_normal(mean = mean, cov=cov, size = n_points[i]))\n",
    "    data = np.concatenate(clusters)\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(n_points[i]):\n",
    "            labels.append(i)\n",
    "        \n",
    "    if add_labels == True:\n",
    "\n",
    "        return data, labels\n",
    "    \n",
    "    else: return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "def plot(X,labels, model, y=None, T=None):\n",
    "    \"\"\"\n",
    "    Plot quantum outcomes.\n",
    "    \n",
    "    Args: \n",
    "        X: training data\n",
    "        y: labels of training data\n",
    "        T: test data. If None, T=X\n",
    "        labels: labels \n",
    "    \"\"\"\n",
    "    \n",
    "    if model=='QNeighborsRegressor' or model=='KNeighborsRegressor':\n",
    "        plt.scatter(X, y, color='darkorange', label='data')\n",
    "        if T is None: T=X\n",
    "        plt.plot(T, labels, color='navy', label='prediction')\n",
    "        plt.axis('tight')\n",
    "        plt.legend()\n",
    "        plt.title(model)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        if np.shape(X)[1] != 2: raise ValueError('Only 2D data can be plotted')\n",
    "        if T is None: T=X\n",
    "        X_data, Y_data = np.hsplit(T,2)\n",
    "        plt.scatter(X_data, Y_data, c = labels.reshape(np.shape(T)[0],-1))\n",
    "        plt.axis('tight')\n",
    "        plt.title(model)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 10px; border-radius: 10px ; border: 1px solid green;\">\n",
    "<font size=\"+2\"><b> 1. Supervised Learning </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present NISQ versions of quantum classifiers and regressors for supervised learning. Our API is built on top of scikit-learn, one of the most widely-used libraries for classical ML. And it's ready to run on your simulator or hardware of your choice! Well, most of them at least!\n",
    "\n",
    "So how can we use NISQ machines for quantum classification in a provably accurate, efficient and explainable way? \n",
    "\n",
    "We developed NISQ versions of canonical classifiers and regressors, based on our Data Loaders and our Distance Metrics procedures. You may not be able to run your real datasets yet on 53-qubit hardware, but one can squeeze all the power out of NISQ machines. On a 64-qubit quantum machine one can a priori perform quantum classification of 64-dimensional real-valued data, with circuit depth less than 15, and less than 130 2-qubit gates. Not too shaby!\n",
    "\n",
    "And of course, one should get ready for higher performance and accuracy as the quantum hardware is getting better and bigger! We can also work together and use our tools to see what type of hardware one would need in order to run domain-specific Supervised Learning applications and also benchmark different hardware technologies.\n",
    "\n",
    "Let us delve into the QML world then!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 1.1 Classification </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**qcware.qml.classification** contains our NISQ quantum classifiers based on similarity learning. In other words, one uses the available labelled data to fit a model that can then predict the labels of new data by finding similarities between them. Some of the canonical classifiers here are the Nearest Centroid and the k-Nearest Neighbors classifiers. \n",
    "\n",
    "For anyone who has used scikit-learn, using the quantum classifiers should be seemless. For the others, it should still be a walk in the park. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a single function call with the following syntax and parameters\n",
    "\n",
    "    fit_and_predict(X, y, T, model, parameters, backend)\n",
    "    \n",
    "        - X: an 2d array holding the training points of size [n_samples, n_features] \n",
    "        - y: an 1d array of class labels of the training points of size [n_samples] \n",
    "        - T: an 2d array holding the test points of size [n_tests, n_features]. \n",
    "             (optional argument) If T is not given, then X is taken as the test data. \n",
    "        - model: 'QNearestCentroid' or 'QNeighborsClassifier' [for classification]\n",
    "        - parameters: a dictionary with each classifier's parameters (details below)\n",
    "        - backend: **all the available backends**\n",
    "\n",
    "The quantum classifier uses the training data (X,Y) to fit the model and predict the labels corresponding to the test data T. \n",
    "\n",
    "We have also created a simple generate_data_clusters function for generating synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 6px; border-radius: 6px ; border: 0px solid green;\">\n",
    "<font size=\"+0\"><b> 1.1.1 QNearestCentroid: The Quantum Nearest Centroid Classifier</b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical Nearest Centroid classifier uses the training points belonging to each class in order to compute the centroid of each class. Then, the label of a test point is chosen by computing the distance of the point to all centroids and assigning the label of the nearest one. This classification is also used within the $k$-means and the Expectation Maximization algorithms for unsupervised learning.\n",
    "\n",
    "Here, we perform the label assignement by using our Distance Estimation quantum subroutines.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "    fit_and_predict(X, y, T, model='QNearestCentroid', parameters, backend)\n",
    "\n",
    "        - X,y,T: the training data, training labels, and test data. \n",
    "        - parameters: {'loader_mode': 'parallel' or 'optimized'}\n",
    "        - backend: **available backends**\n",
    "    \n",
    "The 'parallel' mode uses n qubits and 2logn depth to deal with n-dimensional real-valued data, while the 'optimized' mode provides an optimal tradeoff between qubits and depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's see how easy it all is. Here is how to perform your first real quantum classification. Define your training and test data (we made a quick function for creating synthetic data, but you can use any data you have). Call the fit_and_predict function with 'QNearestCentroid' and get back your results coming from measurements on a quantum circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some synthetic data\n",
    "n_clusters = 4\n",
    "n_points = 10\n",
    "dimension = 2\n",
    "X, y = generate_data_clusters(n_clusters = n_clusters, \n",
    "                              n_points = n_points, \n",
    "                              dimension = dimension, \n",
    "                              add_labels=True)\n",
    "\n",
    "# let's run the quantum classifier\n",
    "qlabels = fit_and_predict(X,y=y,model='QNearestCentroid',  backend='qcware/cpu_simulator')\n",
    "print('Quantum labels\\n',qlabels)\n",
    "\n",
    "#import NearestCentroid from scikit-learn for benchmarking\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "clabels = NearestCentroid().fit(X,y).predict(X)\n",
    "print('Classical labels\\n',clabels)\n",
    "\n",
    "if dimension==2:\n",
    "    plot(X,qlabels,'QNearestCentroid')\n",
    "    plot(X,clabels,'KNearestCentroid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 6px; border-radius: 6px ; border: 0px solid green;\">\n",
    "<font size=\"+0\"><b> 1.1.2 QNeighborsClassifier: The Quantum k-Nearest Neighbors Classifier </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical k-Nearest Neighbors classifier finds the $k$ training points that are closest in Euclidean distance to the new test point and assigns the label that corresponds to the majority one. Nearest Neighbors methods do not provide a model but store all of the training points in a clever data structure that then is used in order to find more efficiently the $k$ neighbours and predict the label of the new test point. \n",
    "\n",
    "Our QNeighborsClassifier uses again our Distance Estimation quantum subroutines to find the nearest neighbors and assign the labels accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "    fit_and_predict(X, Y, T, model=QNeighborsClassifier, parameters, backend)\n",
    "\n",
    "        - X,y,T: the training data, training labels, and test data. \n",
    "        - model: QNeighborsClassifier\n",
    "        - parameters: {'n_neighbors': 3, 'loader_mode': 'parallel' or 'optimized'}\n",
    "        - backend: **avaliable backends**\n",
    "    \n",
    "The 'parallel' mode uses n qubits and 2logn depth to deal with n-dimensional real-valued data, while the 'optimized' mode provides an optimal tradeoff between qubits and depth. The n_neighbors can be used to choose the number of neighbors used for the classification (default value is 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Again, pretty straightforward. Define your training and test data. Call the fit_and_predict function with 'QNeighborsClassifier', and get back your results coming from measurements on a quantum circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some synthetic data\n",
    "n_clusters = 4\n",
    "n_points = 10\n",
    "dimension = 2\n",
    "X, y = generate_data_clusters(n_clusters = n_clusters, \n",
    "                              n_points = n_points, \n",
    "                              dimension = dimension, \n",
    "                              add_labels=True)\n",
    "\n",
    "# let's run the quantum classifier\n",
    "qlabels = fit_and_predict(X,y=y,model='QNeighborsClassifier', parameters={'n_neighbors':3})\n",
    "print('Quantum labels  ',qlabels)\n",
    "\n",
    "\n",
    "#import KNeighborsClassifier from scikit-learn for benchmarking\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clabels = KNeighborsClassifier(n_neighbors = 3).fit(X,y).predict(X)\n",
    "print('Classical labels',clabels)\n",
    "\n",
    "if dimension==2:\n",
    "    plot(X,qlabels,'QNeighborsClassifier')\n",
    "    plot(X,clabels,'KNeighborsClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 1.2 Regression </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**qcware.qml.regression** contains our NISQ quantum regressor, QNeighborsRegressor, based on the k-Nearest Neighbor regressor. These regressor basically works like the k-Nearest Neighbor classifiers while outputing a real value as the label of the data point.\n",
    "\n",
    "The syntax is exactly the same as the QNeighborsClassifier and one can call the fit_and_predict function as\n",
    "\n",
    "    fit_and_predict(X, Y, T, model=QNeighborsClassifier, parameters, backend)\n",
    "\n",
    "        - X: an 2d array holding the training points of size [n_samples, n_features] \n",
    "        - y: an 1d array of class labels of the training points of size [n_samples] \n",
    "        - T: an 2d array holding the test points of size [n_tests, n_features]. \n",
    "             (optional argument) If T is not given, then X is taken as the test data. \n",
    "        - model: QNeighborsRegressor\n",
    "        - parameters: {'n_neighbors': 3, 'loader_mode': 'parallel' or 'optimized'}\n",
    "        - backend: **avaliable backends**\n",
    "    \n",
    "The 'parallel' mode uses n qubits and 2logn depth to deal with n-dimensional real-valued data, while the 'optimized' mode provides an optimal tradeoff between qubits and depth. The n_neighbors can be used to choose the number of neighbors used for the classification (default value is 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's try to run an example from scikit-learn's library. Once again, same procedure. Define your training and test data. Call the fit_and_predict function with 'QNeighborsRegressor', and get back your results coming from measurements on a quantum circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Generate regression data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "# try different functions\n",
    "y = np.sin(X**2).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "# ################################\n",
    "# let's run the quantum regressor QNeighborsRegressor\n",
    "n_neighbors = 5\n",
    "qlabels = fit_and_predict(X,y=y,T=T, model='QNeighborsRegressor', parameters={'n_neighbors':n_neighbors})\n",
    "\n",
    "\n",
    "# #####################################################\n",
    "# Let's run the classical regressor KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "clabels = KNeighborsRegressor(n_neighbors, weights='uniform').fit(X, y).predict(T)\n",
    "\n",
    "\n",
    "# ######################\n",
    "# Let's plot the results\n",
    "\n",
    "plot(X,qlabels,'QNeighborsRegressor',y=y,T=T)\n",
    "plot(X,clabels,'KNeighborsRegressor',y=y,T=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 10px; border-radius: 10px ; border: 1px solid green;\">\n",
    "<font size=\"+2\"><b> 2. Unsupervised Learning </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present NISQ versions of quantum clustering for unsupervised learning. Our API is built on top of scikit-learn, one of the most widely-used libraries for classical ML. And it's ready to run on your simulator or hardware of your choice! Well, most of them at least!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 2.1 Clustering </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**qcware.qml.clustering** provides NISQ versions of quantum clustering algorithms for unsupervised learning. \n",
    "\n",
    "In this release, we developed NISQ versions of the canonical KMeans clustering algorithm, based on our Data Loaders and our Distance Metrics procedures. Once again, you may not be able to run your real datasets just yet, but, for the time being, one can make the best out of the NISQ machines and get ready for higher performance and accuracy as the quantum hardware improves. We can also work together and use our tools to see what type of hardware one would need in order to run domain-specific clustering applications and also benchmark different hardware technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the same function call with the following syntax and parameters\n",
    "\n",
    "    fit_and_predict(X, T, model, parameters, backend)\n",
    "    \n",
    "        - X: an 2d array holding the training points of size [n_samples, n_features]  \n",
    "        - T: an 2d array holding the test points of size [n_tests, n_features]. \n",
    "            (optional argument) If T is not given, then X is taken as the test data. \n",
    "        - model: QMeans\n",
    "        - parameters: {n_clusters, loader_mode = 'parallel' or 'optimized'}\n",
    "        - backend: **all the available backends**\n",
    "\n",
    "Note that there are no labels y in unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Last Example for the time being. Define your training data. Call the fit_and_predict function with 'QMeans', and get back your results coming from measurements on a quantum circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some synthetic data\n",
    "n_clusters = 4\n",
    "n_points = 10\n",
    "dimension = 2\n",
    "X = generate_data_clusters(n_clusters = n_clusters, \n",
    "                              n_points = n_points, \n",
    "                              dimension = dimension, \n",
    "                              add_labels=False)\n",
    "\n",
    "# let's run the quantum regressor\n",
    "qlabels = fit_and_predict(X,y=y,model='QMeans', parameters={'n_clusters': n_clusters, 'loader_mode':'parallel'})\n",
    "print('Quantum labels  ', qlabels)\n",
    "\n",
    "#import KMeans from scikit-learn for benchmarking\n",
    "from sklearn.cluster import KMeans\n",
    "clabels = KMeans(n_clusters=n_clusters).fit(X).predict(X)\n",
    "print('Classical labels',clabels)\n",
    "\n",
    "if dimension==2:\n",
    "    plot(X,qlabels,'QMeans')\n",
    "    plot(X,clabels,'KMeans')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
