{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 20px; border-radius: 10px ; border: 2px solid green;\">\n",
    "<p><font size=\"+3\"><b><center> qcware.qML </center></b></font>\n",
    "</p>\n",
    "<p>\n",
    "<font size=\"+3\">  <b> <center> The Quantum Machine Learning API</center></b></font>\n",
    "</p>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**qcware.qML** is Forge's API for Machine Learning applications. \n",
    "\n",
    "Before going any further, make sure you have run the **Getting Started** notebook, to install the qml library and learn about our data loaders and distance estimation procedures that power our qml API. Once you are done with that, you can go ahead with our Supervised Learning functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 10px; border-radius: 10px ; border: 1px solid green;\">\n",
    "<font size=\"+2\"><b> 1. Supervised Learning </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present NISQ versions of quantum classifiers and regressors for supervised learning. Our API is built on top of scikit-learn, one of the most widely-used libraries for classical ML. And it's ready to run on your simulator or hardware of your choice! Well, most of them at least!\n",
    "\n",
    "So how can we use NISQ machines for quantum classification in a provably accurate, efficient and explainable way? \n",
    "\n",
    "We developed NISQ versions of canonical classifiers and regressors, based on our Data Loaders and our Distance Metrics procedures. You may not be able to run your real datasets yet on 53-qubit hardware, but one can squeeze all the power out of NISQ machines. On a 64-qubit quantum machine one can a priori perform quantum classification of 64-dimensional real-valued data, with circuit depth less than 15, and less than 130 2-qubit gates. Not too shaby!\n",
    "\n",
    "And of course, one should get ready for higher performance and accuracy as the quantum hardware is getting better and bigger! We can also work together and use our tools to see what type of hardware one would need in order to run domain-specific Supervised Learning applications and also benchmark different hardware technologies.\n",
    "\n",
    "Let us delve into the QML world then!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 1.1 Classification </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**qcware.qml.classification** contains our NISQ quantum classifiers based on similarity learning. In other words, one uses the available labelled data to fit a model that can then predict the labels of new data by finding similarities between them. Some of the canonical classifiers here are the Nearest Centroid and the k-Nearest Neighbors classifiers. \n",
    "\n",
    "For anyone who has used scikit-learn, using the quantum classifiers should be seemless. For the others, it should still be a walk in the park. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a single function call with the following syntax and parameters\n",
    "\n",
    "    fit_and_predict(X, y, T, model, parameters, backend)\n",
    "    \n",
    "        - X: an 2d array holding the training points of size [n_samples, n_features] \n",
    "        - y: an 1d array of class labels of the training points of size [n_samples] \n",
    "        - T: an 2d array holding the test points of size [n_tests, n_features]. \n",
    "             (optional argument) If T is not given, then X is taken as the test data. \n",
    "        - model: 'QNearestCentroid' or 'QNeighborsClassifier' [for classification]\n",
    "        - parameters: a dictionary with each classifier's parameters (details below)\n",
    "        - backend: **all the available backends**\n",
    "\n",
    "The quantum classifier uses the training data (X,Y) to fit the model and predict the labels corresponding to the test data T. \n",
    "\n",
    "We have also created a simple generate_data_clusters function for generating synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 6px; border-radius: 6px ; border: 0px solid green;\">\n",
    "<font size=\"+0\"><b> 1.1.1 QNearestCentroid: The Quantum Nearest Centroid Classifier</b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical Nearest Centroid classifier uses the training points belonging to each class in order to compute the centroid of each class. Then, the label of a test point is chosen by computing the distance of the point to all centroids and assigning the label of the nearest one. This classification is also used within the $k$-means and the Expectation Maximization algorithms for unsupervised learning.\n",
    "\n",
    "Here, we perform the label assignement by using our Distance Estimation quantum subroutines.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "    fit_and_predict(X, y, T, model='QNearestCentroid', parameters, backend)\n",
    "\n",
    "        - X,y,T: the training data, training labels, and test data. \n",
    "        - parameters: {'loader_mode': 'parallel' or 'optimized'}\n",
    "        - backend: **available backends**\n",
    "    \n",
    "The 'parallel' mode uses n qubits and 2logn depth to deal with n-dimensional real-valued data, while the 'optimized' mode provides an optimal tradeoff between qubits and depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's see how easy it all is. Here is how to perform your first real quantum classification. Define your training and test data (we made a quick function for creating synthetic data, but you can use any data you have). Call the fit_and_predict function with 'QNearestCentroid' and get back your results coming from measurements on a quantum circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some synthetic data\n",
    "from generate_data_clusters import generate_data_clusters\n",
    "from plot import plot\n",
    "from qcware.qml import fit_and_predict\n",
    "n_clusters = 4\n",
    "n_points = 10\n",
    "dimension = 2\n",
    "X, y = generate_data_clusters(n_clusters = n_clusters, \n",
    "                              n_points = n_points, \n",
    "                              dimension = dimension, \n",
    "                              add_labels=True)\n",
    "\n",
    "# let's run the quantum classifier\n",
    "qlabels = fit_and_predict(X,y=y,model='QNearestCentroid',  backend='qcware/cpu_simulator')\n",
    "print('Quantum labels\\n',qlabels)\n",
    "\n",
    "#import NearestCentroid from scikit-learn for benchmarking\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "clabels = NearestCentroid().fit(X,y).predict(X)\n",
    "print('Classical labels\\n',clabels)\n",
    "\n",
    "if dimension==2:\n",
    "    plot(X,qlabels,'QNearestCentroid')\n",
    "    plot(X,clabels,'KNearestCentroid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 6px; border-radius: 6px ; border: 0px solid green;\">\n",
    "<font size=\"+0\"><b> 1.1.2 QNeighborsClassifier: The Quantum k-Nearest Neighbors Classifier </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical k-Nearest Neighbors classifier finds the $k$ training points that are closest in Euclidean distance to the new test point and assigns the label that corresponds to the majority one. Nearest Neighbors methods do not provide a model but store all of the training points in a clever data structure that then is used in order to find more efficiently the $k$ neighbours and predict the label of the new test point. \n",
    "\n",
    "Our QNeighborsClassifier uses again our Distance Estimation quantum subroutines to find the nearest neighbors and assign the labels accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "    fit_and_predict(X, Y, T, model=QNeighborsClassifier, parameters, backend)\n",
    "\n",
    "        - X,y,T: the training data, training labels, and test data. \n",
    "        - model: QNeighborsClassifier\n",
    "        - parameters: {'n_neighbors': 3, 'loader_mode': 'parallel' or 'optimized'}\n",
    "        - backend: **avaliable backends**\n",
    "    \n",
    "The 'parallel' mode uses n qubits and 2logn depth to deal with n-dimensional real-valued data, while the 'optimized' mode provides an optimal tradeoff between qubits and depth. The n_neighbors can be used to choose the number of neighbors used for the classification (default value is 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Again, pretty straightforward. Define your training and test data. Call the fit_and_predict function with 'QNeighborsClassifier', and get back your results coming from measurements on a quantum circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some synthetic data\n",
    "n_clusters = 4\n",
    "n_points = 10\n",
    "dimension = 2\n",
    "X, y = generate_data_clusters(n_clusters = n_clusters, \n",
    "                              n_points = n_points, \n",
    "                              dimension = dimension, \n",
    "                              add_labels=True)\n",
    "\n",
    "# let's run the quantum classifier\n",
    "qlabels = fit_and_predict(X,y=y,model='QNeighborsClassifier', parameters={'n_neighbors':3})\n",
    "print('Quantum labels  ',qlabels)\n",
    "\n",
    "\n",
    "#import KNeighborsClassifier from scikit-learn for benchmarking\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clabels = KNeighborsClassifier(n_neighbors = 3).fit(X,y).predict(X)\n",
    "print('Classical labels',clabels)\n",
    "\n",
    "if dimension==2:\n",
    "    plot(X,qlabels,'QNeighborsClassifier')\n",
    "    plot(X,clabels,'KNeighborsClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 1.2 Regression </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**qcware.qml.regression** contains our NISQ quantum regressor, QNeighborsRegressor, based on the k-Nearest Neighbor regressor. These regressor basically works like the k-Nearest Neighbor classifiers while outputing a real value as the label of the data point.\n",
    "\n",
    "The syntax is exactly the same as the QNeighborsClassifier and one can call the fit_and_predict function as\n",
    "\n",
    "    fit_and_predict(X, Y, T, model=QNeighborsClassifier, parameters, backend)\n",
    "\n",
    "        - X: an 2d array holding the training points of size [n_samples, n_features] \n",
    "        - y: an 1d array of class labels of the training points of size [n_samples] \n",
    "        - T: an 2d array holding the test points of size [n_tests, n_features]. \n",
    "             (optional argument) If T is not given, then X is taken as the test data. \n",
    "        - model: QNeighborsRegressor\n",
    "        - parameters: {'n_neighbors': 3, 'loader_mode': 'parallel' or 'optimized'}\n",
    "        - backend: **avaliable backends**\n",
    "    \n",
    "The 'parallel' mode uses n qubits and 2logn depth to deal with n-dimensional real-valued data, while the 'optimized' mode provides an optimal tradeoff between qubits and depth. The n_neighbors can be used to choose the number of neighbors used for the classification (default value is 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's try to run an example from scikit-learn's library. Once again, same procedure. Define your training and test data. Call the fit_and_predict function with 'QNeighborsRegressor', and get back your results coming from measurements on a quantum circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Generate regression data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "# try different functions\n",
    "y = np.sin(X**2).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "# ################################\n",
    "# let's run the quantum regressor QNeighborsRegressor\n",
    "n_neighbors = 5\n",
    "qlabels = fit_and_predict(X,y=y,T=T, model='QNeighborsRegressor', parameters={'n_neighbors':n_neighbors, 'num_measurements': 10000})\n",
    "\n",
    "# #####################################################\n",
    "# Let's run the classical regressor KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "clabels = KNeighborsRegressor(n_neighbors, weights='uniform').fit(X, y).predict(T)\n",
    "\n",
    "\n",
    "# ######################\n",
    "# Let's plot the results\n",
    "\n",
    "plot(X,qlabels,'QNeighborsRegressor',y=y,T=T)\n",
    "plot(X,clabels,'KNeighborsRegressor',y=y,T=T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
