{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 20px; border-radius: 10px ; border: 2px solid green;\">\n",
    "<p><font size=\"+3\"><b><center> qcware.qML </center></b></font>\n",
    "</p>\n",
    "<p>\n",
    "<font size=\"+3\">  <b> <center> The Quantum Machine Learning API</center></b></font>\n",
    "</p>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**qcware.qML** is Forge's API for Machine Learning applications. \n",
    "\n",
    "Our ambition is to deliver the first real Quantum Machine Learning applications by bridging the gap between higly impactful QML algorithms and the NISQ era quantum machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why QML?** \n",
    "\n",
    "First because ML provides extremely impactful tools for a number of different domains (Finance, Healthcare, Automotive, etc.), so it only makes sense to try as hard as possible and see how quantum can enhance these tools to offer Efficiency, Accuracy, Explainability, Trustworthiness, Energy savings etc. \n",
    "\n",
    "Second, we do know fault tolerant quantum computers can offer big advantages in ML, and we have contributed largely in the growing body of work in this area. \n",
    "\n",
    "Third, because we do have concrete avenues for bringing QML towards the NISQ era: by reducing resource requirements of impactful QML algorithms; by proposing NISQ Quantum Deep Learning with provable guarantees; even by pushing the boundaries of NISQ technologies to new QML-specific hardware architectures for overcoming bottlenecks.\n",
    "\n",
    "Our QML methodology is NOT to nullify the algorithmic intuition and use blindly current hardware. It is both to push the boundaries of NISQ hardware and to reduce the algorithmic needs to get to the first real-world QML applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we do this?**\n",
    "\n",
    "We needed to first solve a number of bottlenecks and get a brand new engine under the hood. How do we efficiently load classical data into quantum states? How do we extract useful information from quantum solutions fast? How do we calculate the similarity between data points, users, stock histories, patients on the fly? These are our propriatory \"under the hood\" developments that make our API stronger than ever and ready for a test drive. \n",
    "\n",
    "In the current release, we use our new powerful quantum tools for supervised and unsupervised learning and provide NISQ solutions for classification, regression and clustering.\n",
    "\n",
    "The target audience of Forge is both the users who want to solve directly their use cases using our optimized quantum functionalities without having to worry too much about the quantum tools underneath, and also the users who want to start experimenting by themselves with building small quantum circuits and functions. \n",
    "\n",
    "But the best way to do this is to do this together! Some of our more powerful propriatory quantum tools make sense to be available in a more \"supervised\" setting, where QC Ware quantum algorithm experts accompany the users through Proof of Concept projects to step by step understand the power and limitations of these tools and how they can be optimally applied to each specific use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qcware.qml import fit_and_predict\n",
    "# as always when using Forge, we must set our API key if running outside the hosted notebooks\n",
    "import qcware\n",
    "# qcware.config.set_api_key('QCWARE')\n",
    "# if scikit-learn is not installed, install it; it should be installed on the Forge notebook servers\n",
    "!pip install sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 10px; border-radius: 10px ; border: 1px solid green;\">\n",
    "<font size=\"+2\"><b> 0. Under the hood </b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's take a peek on what we built under the hood to ensure that we move closer towards these first QML applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 0.5px solid green;\">\n",
    "<font size=\"+1\"><b> 0.1 quasar </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**quasar** is our internal quantum language which for a user who has experimented with other quantum languages will seem very intuitive and similar to use. It is a simple circuit writing tool that offers some big advantages: \n",
    "\n",
    "- Design your circuit once, run it on all available simulators and quantum hardware \n",
    "- Use powerful built-in functionalities specifically for Optimization, Chemistry and ML\n",
    "- Multiple backends supported\n",
    "- GPU powered simulators available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 0.2 Data loaders </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first obstacles towards practical QML was loading efficiently classical data onto quantum states that can be used for further computation.\n",
    "Why do we need that? Well, because ML data IS and will predominantly remain classical (images, texts, preferences, stock market, internet data,...) so if we are looking for impactful quantum applications, ignoring classical data is not an option!\n",
    "\n",
    "So what do we mean by **efficiently** loading classical data onto quantum states? We certainly  do not want to assume any exotic hardware technologies that are not or will not be available in the NISQ era. And more importantly we want to be able to create amplitude encodings of our classical data points as fast as possible, namely we want to be able to load classical data in the following sense:  \n",
    "\n",
    "    - Read the classical data once! (time: O(n), for n-dimensional real-valued data)\n",
    "    - Prepare quantum encodings of the data fast! (time: logn, circuit size (2-q gates): n)\n",
    "\n",
    "And we deliver exactly that. We have designed data loaders that:\n",
    "\n",
    "- loader(): Provide the **optimal** ways for loading classical data onto quantum states\n",
    "- Can be deployed using current NISQ machines\n",
    "- Can be readily used as subroutines to bring many QML algorithms closer to the NISQ era.  \n",
    "\n",
    "We are also discussing with quantum hardware collaborators to design tailor-made hardware chips for even better performance and seemless integration. Stay tuned!\n",
    "\n",
    "Our standard Forge users will be able to get advantage of our data loaders through the qML functionalities that we are offering both in the supervised and unsupervised learning. For the ones who would really want to spend more time and effort to understand the inner workings of the loaders and our other quantum tools, we are happy to accompany you on this journey. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Here is how the loaders work in very high level. Define your data point x as an 1d array. Normalize it. Call the loader to produce the circuit to create the quantum amplitude encoding of x. As simple as that. You can see how many qubits, what depth or how many gates the loader uses. Here is some sample code of what we do under the hood. These loaders are going to be used when you run your own QML applications in the sections below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # create a synthetic data point or upload your own data!\n",
    "\n",
    "    import numpy as np\n",
    "    x = np.random.rand(64)\n",
    "    x = x/np.linalg.norm(x)\n",
    "\n",
    "    # create a quasar circuit and call the loader function.\n",
    "    # The loader function takes as input the classical data point\n",
    "    # and mode 'parallel' or 'optimized' and it returns the quantum circuit\n",
    "\n",
    "    parallel_loader = qio.loader(x, mode='parallel')\n",
    "\n",
    "    # let's see what characteristics the circuit has\n",
    "    print(\"Parallel loader characteristics\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"number of qubits:\",parallel_loader.nqubit)\n",
    "    print(\"circuit depth   :\",parallel_loader.ntime)\n",
    "    print(\"number of gates :\",parallel_loader.ngate,\"\\n\")\n",
    "\n",
    "\n",
    "    # let's do it for the 'optimized' loader as well\n",
    "\n",
    "    optimized_loader = qio.loader(x,mode='optimized')\n",
    "\n",
    "    print(\"Optimized loader characteristics\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"number of qubits:\",optimized_loader.nqubit)\n",
    "    print(\"circuit depth   :\",optimized_loader.ntime)\n",
    "    print(\"number of gates :\",optimized_loader.ngate,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the characteristics of the loaders for 64-dimensional data\n",
    "    \n",
    "    >>> Parallel loader characteristics\n",
    "    >>> -------------------------------\n",
    "    >>> number of qubits: 64\n",
    "    >>> circuit depth   : 7\n",
    "    >>> number of gates : 64 \n",
    "    >>> \n",
    "    >>> Optimized loader characteristics\n",
    "    >>> -------------------------------\n",
    "    >>> number of qubits: 16\n",
    "    >>> circuit depth   : 34\n",
    "    >>> number of gates : 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 0.3 Distance Metrics Estimation </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason to create these data loaders is because they unlock a throve of important capabilities, the first of which is estimating the similarity or distance between data points in a fast way. Distance metrics are at the core of Similarity Learning, which is one of the fundamental branches of Supervised and Unsupervised Learning that provides accurate, efficient, and explainable AI applications.\n",
    "\n",
    "Our Distance Metrics estimation functionalities combine in a clever way our loaders, and not much more, to create NISQ subroutines for Similarity Learning. More precisely, we have developed distance metrics subroutines that:\n",
    "\n",
    "- distance_estimation(): Provide **optimal** Euclidean distance estimation of data points\n",
    "- qdot(): Provide **optimal** Inner Product estimation between data points\n",
    "- Can be deployed using current NISQ machines\n",
    "- Can be readily used as subroutines to bring many QML algorithms closer to the NISQ era.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "Here it is how it works. Define your data points x and y as 1d arrays. Call the distance_estimation prodecure to get an estimate of their squared Euclidean distance. The distance_estimation procedure constructs a quantum circuit and uses the designated backend (here the quasar simulator) to run the quantum circuit, get measurement results and output the estimate. Again, these circuits are going to be used when you run your own QML applications in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # create two random data points or load your own data\n",
    "    \n",
    "    import numpy as np\n",
    "    x = np.random.rand(16)\n",
    "    y = np.random.rand(16)\n",
    "\n",
    "    # let's estimate the distance and compare it to the real one\n",
    "    \n",
    "    print('real distance',np.linalg.norm(x-y)**2)\n",
    "    print('distance est.',qutils.distance_estimation(x, y,loader_mode='parallel'))\n",
    "\n",
    "    # we can also see what characteristics the circuit has\n",
    "\n",
    "    print(\"\\nDistance Estimation characteristics\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"number of qubits:\",dist_est.nqubit)\n",
    "    print(\"circuit depth:\",dist_est.ntime)\n",
    "    print(\"number of gates\",dist_est.ngate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample output and the characteristics of the quantum circuit for 16-dimensional data\n",
    "    \n",
    "    >>> real distance 2.9635211340099668\n",
    "    >>> distance est. 2.96147527209646\n",
    "    >>>\n",
    "    >>> Distance Estimation characteristics\n",
    "    >>> -------------------------------\n",
    "    >>> number of qubits: 17\n",
    "    >>> circuit depth: 13\n",
    "    >>> number of gates 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the inner product between vectors or matrices in the same way. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # create two random data matrices or load your own data\n",
    "    import numpy as np\n",
    "    x = np.random.rand(4,5)\n",
    "    y = np.random.rand(5,3)\n",
    "\n",
    "    # compute the dot product of the two matrices\n",
    "    # and the quantum dot product estimation of the two matrices\n",
    "    \n",
    "    print(np.dot(x,y))\n",
    "    print(qutils.qdot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample outcome\n",
    "    \n",
    "    >>> [[1.68310796 0.95850939 1.23604967]\n",
    "    >>>  [1.71744922 1.08271752 1.4903697 ]\n",
    "    >>>  [1.01983769 0.58927693 0.74201257]\n",
    "    >>>  [0.61285301 0.24302302 0.88084614]]\n",
    "    >>> [[1.68568646 0.95377326 1.25345586]\n",
    "    >>>  [1.71279657 1.07006217 1.4773043 ]\n",
    "    >>>  [1.01892272 0.58610302 0.74277502]\n",
    "    >>>  [0.61225585 0.24286683 0.88671553]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cfc ; padding: 8px; border-radius: 8px ; border: 1px solid green;\">\n",
    "<font size=\"+1\"><b> 0.4 Datasets and Visualization </b></font>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of ways to load datasets and visualize the results. Any method that works with scikit-learn will also work with our QML algorithms. Be careful, quantum simulations run out of memory pretty fast!\n",
    "\n",
    "Here we just provide some simple code for generating synthetic data and plotting the results. Nothing extraordinary, but an easy way to start playing with our QML functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data_clusters(n_clusters = 4, n_points = 8, dimension = 2, magnitude = 1, spread = 0.05, min_distance = 0.3, add_labels = False):\n",
    "    \"\"\"\n",
    "    Generates data clusters containing npoints, with random centres and spreads.\n",
    "    \n",
    "    Args:\n",
    "     n_clusters (int): Number of clusters to create\n",
    "     n_points (numpy.array): array of number of points in each cluster. \n",
    "                             If int, same number of points in each cluster\n",
    "     dimension (int): Number of features in data.\n",
    "     magnitude: max magnitude of data points\n",
    "     spread: spread of the normal distribution\n",
    "     min_dist: minimum distance between cluster centers\n",
    "     add_labels: True for supervised data, False for unsupervised data\n",
    "    \"\"\"\n",
    "    if type(n_points) == int:\n",
    "        n_points = np.tile(n_points,n_clusters)\n",
    "     \n",
    "    clusters = []\n",
    "    \n",
    "    if min_distance > 0:\n",
    "        means = []\n",
    "        while len(means) < n_clusters:\n",
    "            mean = np.random.random(dimension) * magnitude \n",
    "            bools = []\n",
    "\n",
    "            for i in range(len(means)):\n",
    "                if np.linalg.norm(mean - means[i]) > min_distance:\n",
    "                   bools.append(True)\n",
    "                else: bools.append(False)\n",
    "            if bools.count(False) > 0: \n",
    "                pass\n",
    "            else: \n",
    "                means.append(mean)\n",
    "                 \n",
    "    else: means = np.random.random((n_clusters,dimension)) * magnitude\n",
    "                \n",
    "    for i in range(n_clusters):\n",
    "        mean = np.array(means)[i]\n",
    "        cov = np.identity(dimension) * spread\n",
    "        clusters.append(np.random.multivariate_normal(mean = mean, cov=cov, size = n_points[i]))\n",
    "    data = np.concatenate(clusters)\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(n_points[i]):\n",
    "            labels.append(i)\n",
    "        \n",
    "    if add_labels == True:\n",
    "\n",
    "        return data, labels\n",
    "    \n",
    "    else: return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "def plot(X,labels, model, y=None, T=None):\n",
    "    \"\"\"\n",
    "    Plot quantum outcomes.\n",
    "    \n",
    "    Args: \n",
    "        X: training data\n",
    "        y: labels of training data\n",
    "        T: test data. If None, T=X\n",
    "        labels: labels \n",
    "    \"\"\"\n",
    "    \n",
    "    if model=='QNeighborsRegressor' or model=='KNeighborsRegressor':\n",
    "        plt.scatter(X, y, color='darkorange', label='data')\n",
    "        if T is None: T=X\n",
    "        plt.plot(T, labels, color='navy', label='prediction')\n",
    "        plt.axis('tight')\n",
    "        plt.legend()\n",
    "        plt.title(model)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        if np.shape(X)[1] != 2: raise ValueError('Only 2D data can be plotted')\n",
    "        if T is None: T=X\n",
    "        X_data, Y_data = np.hsplit(T,2)\n",
    "        plt.scatter(X_data, Y_data, c = labels.reshape(np.shape(T)[0],-1))\n",
    "        plt.axis('tight')\n",
    "        plt.title(model)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
